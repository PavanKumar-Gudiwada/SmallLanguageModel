{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "crNB-5sWdhSi"
   },
   "source": [
    "# Training a Small Language Model (SLM) from scratch\n",
    "\n",
    "An SLM with 15M parameters is trained on the TinyStories dataset (contains stories for 3-4 year olds, constructed from GPT4) to generate a story from a prompt.\n",
    "\n",
    "SLMs are not trained on large datasets instead it is trained on a small/subset of the entire data set, for a specific purpouse. Goal is to have a small model learn from this small dataset, understanding the english grammar and some meaning to be able to come up with stories on its own. The model learns to output coherent stories/text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MJKpmqRsfOGR"
   },
   "source": [
    "## Step 1: load datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9690,
     "status": "ok",
     "timestamp": 1755085300786,
     "user": {
      "displayName": "Pavan Kumar",
      "userId": "06100093160327977602"
     },
     "user_tz": -330
    },
    "id": "vcucKXtCdgd5",
    "outputId": "30801832-222c-4142-8652-488675968857"
   },
   "outputs": [],
   "source": [
    "# !pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 402,
     "referenced_widgets": [
      "5ccec7d717e7425294de0a5a42cb259a",
      "6b6e74488cd64a51a84f2f98b832cd04",
      "8aa73e7ee1fa49e492a0438b4c6c2f56",
      "b8d6add63eed4518810a9123ea12f610",
      "093334b702314e7a93da3e50c10e9f46",
      "63ed76cdd49b45548473e6a632898783",
      "dbd80fc1938f49c28d28fa4af8395424",
      "22db7244ab484970a530855407bab846",
      "45fd4b13b49d4eaf8aa4701ca57fca37",
      "f175443a10034c3db31f8fd2958d168d",
      "ddcdea2d7bc049e8b1265919cff1f232",
      "6d6c13fa957f473b8f2fe24ba9a32b58",
      "c4a3ed1d7d5f4d8790545cfdfd96a080",
      "70338f6746fe4f9aa5d11feea1c66b1e",
      "c33878c5c6a542ddb02fd90fbf4eec85",
      "077d5c3022e94da68a26863e4e720229",
      "7e7028efaf1e40f99cda27af49b38d50",
      "24c5c2af1c8747c4918fb0ab1a06a6a7",
      "434f37cef42b41c6ae701dd0ed4c9a42",
      "4991d7beb36a4ac7a388a6bfb757f526",
      "c5c2b964ddc145a88809ded04ffb2bb2",
      "127aebf7dbf44b85b8de7c25ad9f3756",
      "4c6f8a6f2b254838be88aa43f17f7c2e",
      "776007c786ff4c9cada52a9d540b46a3",
      "90e7892dd0e84c28b26cfd4287616d6f",
      "e9317fd2871744c39f73dc5d76062194",
      "b00c051106d343d9a5339092b37d438e",
      "f4f939ea80f0420395891264813ccfb6",
      "0d78cb2e86fc4b4e935b5897e45ce5cd",
      "b4478b2a7f8d4f7682d53379b1e945ca",
      "bc56ba2038d84b4ab06b8553bfe9b257",
      "b582ec7bc5bb425c84bbf8bc846c2eaa",
      "46bdc087d7484c52ae3d4066919a2558",
      "cf01756281e346ec995538c3370c9231",
      "c5acef8d809e4915a9ec7a57f9c7f323",
      "7629d28acf7c4eb7bd617aba644dcee8",
      "27e722c35bbb4b5dad4a62960c94fd89",
      "128c1224c7ca4e5dac9ec78f15b08512",
      "a9c943b9e3f94ffd97fae7faf3235f57",
      "ca19d7696fee4fe68460df99ce0a0faf",
      "916ad561eb77412aa14482edee33698a",
      "b1bab51a59304ec69378cf31df66e33a",
      "0ce18dbe5919480fbbd13d994e51f242",
      "7413d1ffc2ef4b4d824a61ae06b10d62",
      "f30ceb7fee6e46fdbebfc44a2a61c837",
      "0d726c60640043b7af3b5a8da9db01fa",
      "88427f34a5234f07aa11e2bf37970083",
      "3f67afb46a3c4d7580b73c4b9bc3af7b",
      "d1a5771489b94fa285f35c782fe9028c",
      "e1e6ef293c304e3aba9b737b697028eb",
      "33d81ad7150f41de85331dac478e0af8",
      "4a782f88c7de4f2da36da2f184ca789a",
      "de3b03cb05164b0c92360c7cdf62c613",
      "9f163ccd29ce4dfca54d11ae1bfa7b76",
      "e4986175e83247e5871ed6f58f2f0ea9",
      "03984ea4a35b46d89070ef604fb1aa20",
      "b881555506fd41259621d4e08bc25374",
      "b7423839edae467995ccb9d6308b0507",
      "a75612b67f424de6a13ba181c892da74",
      "977d0f79e4c341f685d304ae3f783d3e",
      "956f38d7b2b3470e9db7790a995ebcab",
      "b604e45538424756a8ccc02852e4eba9",
      "d601e665575947459d993e6b694a8cda",
      "80f3a4400bf14d269a86f3afab0b2461",
      "32142373841a4870a39dcdb039c1ceb3",
      "3ee479df834f4e52832e24afde5d2f68",
      "0ab5a7f36700419fb7c1df074b9da1a2",
      "684c3e4a2797440d99e6dc0f694cc9cd",
      "9e4c3386141e4d03a04498a710b1c866",
      "77883ebee79843008291906ba90be6eb",
      "c05a35f94d9a48e791aa32aa547f572f",
      "ca24583cc3cd439b995caef6b02bfc45",
      "9411271086fd4f9cb57f3d85293418a6",
      "f8e934b9b42343119204d2134b2697d4",
      "133a610a21c44bf6b2d7328da79afaab",
      "fc2d84ae6b134faa9506d32b8f7d3b1a",
      "6641dd02e91140e9b6225cb00ec29ec7",
      "c359f8276baf45f1aedb8133714d14e8",
      "8fc95b40908e4b0497ea8fc0e47aea82",
      "4ebaaa9bf8c24d358fe53bf1f5baae02",
      "0fb97731871146c18ffe6f2f592366bf",
      "c135252ecf534cde9e603a707ca3f61d",
      "abf63e7d28a64c9da4778307ab7ba877",
      "e1c7ff90df9446d29cc096d80c5feac8",
      "65289dff1ce449b482952d9f823b72f0",
      "923315fced6f468b8dfc1ce69ab6f7dc",
      "1ac5343504b74b92a1d7bcea9f4a92e0",
      "76a6030fbdc44501811b0110c30332e4"
     ]
    },
    "executionInfo": {
     "elapsed": 56738,
     "status": "ok",
     "timestamp": 1755085357538,
     "user": {
      "displayName": "Pavan Kumar",
      "userId": "06100093160327977602"
     },
     "user_tz": -330
    },
    "id": "QePu5KzRjYUy",
    "outputId": "fc87e007-fdfe-425b-8886-b698e0ae1d8b"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"roneneldan/TinyStories\")\n",
    "\n",
    "# Choose a custom cache dir so we know where it lives\n",
    "# ds = load_dataset(\"roneneldan/TinyStories\", cache_dir=\"./hf_cache\")\n",
    "\n",
    "# # Optional: save as Arrow or JSON for fully offline use later\n",
    "# ds.save_to_disk(\"./tiny_stories_local\")\n",
    "\n",
    "# ds = load_from_disk(\"./tiny_stories_local\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4dBczG8WkJk9"
   },
   "source": [
    "## step 2: Data pre-processing\n",
    "\n",
    "Since neural networks or computers understand only numerical data, convert the sentences/words/tokens into vectors.\n",
    "\n",
    "1. Word based tokenisation (vocabulary/tabel stored for each word/token vs the token ids) (vocabulary size plays an important role in resources used)\n",
    "\n",
    "2. Subword tokenisation (tokens can be words, characters or subwords)\n",
    "  Byte pair encoding (Tokeniser used here), also used by GPT2.\n",
    "\n",
    "3. Character based tokenisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zTrsrXnbnLjQ"
   },
   "source": [
    "Tokenising here is done for every story and instead of maintaining a dictionaly of token ids for each story, the tokens are merged and stored in a .bin file for all sories together. .bin file has the advantage of being stored on the disk directly instead of using the RAM. Not using .bin file here slows down our code and is one of the techniques used to speed up our code. Also we do not need to retokenise each time we want to train our model as it is already stored on our disk.\n",
    "\n",
    "We then creat/use a memory mapped array to read/write into disk, without overloading the RAM.\n",
    "We create np.memmap, i.e. file is stored on disk, but can be used like a numpy array, where we can write to it chunk by chunk wihout storing everything in the ram.\n",
    "\n",
    "The entire traning dataset of 2.12M is then made into 1024 batches and then the token ids from each batch of stories is put into a 'train.bin' file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DJIz1e1Pp1Mk"
   },
   "source": [
    "tiktoken is a OpenAI library to get tokeniser/encodings used for different GPT models.\n",
    "\n",
    "At end of this block\n",
    "1. We have tokenised the dataset into tokens\n",
    "2. Create files called 'train.bin' and 'validation.bin' containing token Ids from the respective parts of the entire dataset.\n",
    "3. We store tokenIds on disk rather than on RAM for efficient computations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 181,
     "referenced_widgets": [
      "814271c858e34b3d83e01a0b6b3c8782",
      "415d847a51af4e4993968bec26fd52fc",
      "74aacaddf94443b089d320cc3485dc6f",
      "b9dde788a0974bd9a65f2f54cc3b7ff0",
      "3e4da415be454f1895b45ea98cd91753",
      "e227034e5ae442c396576f47cd4f62f4",
      "825c5c12e39c49cc81b2028d0bf4194e",
      "d33d44a88eb14ce59392b30996e99eb7",
      "871eb5db017a4d9387d5377103e61b4f",
      "bb2da629970f4d44af1f7b86f5307284",
      "23dd05a3f3bb49fc9382d6629aa26f71",
      "a0c3583d8e064a429e17c7d38dc12543",
      "f53120af6293413bad4ac62ac19450a3",
      "8a0b4885f2f44300be4f1461da88a613",
      "2cf2359002dd48c18058b2ed69192e62",
      "aa83f7ab980a48a5a1953d4d64d0f8dc",
      "053d88e6ab6e4a379720f400773c0197",
      "3c65770daf8249a98e4b3f4076ef9ff7",
      "9c8bd3fec37546729aef786bae649edd",
      "921c01ee88684e69a83fbfd4de78ae97",
      "ccc70302cfde4880a76fc99326567f43",
      "9eceb2d59af24225967b940f5e4db0c7",
      "a35f4b1e73d64400a1f6961a6b9973ca",
      "13b1bbdd6905438586b508294b8555ba",
      "b98c13bb924b463d91adedd3ce44f75b",
      "ecc77b34a5a043f996ac3cf0a4863c03",
      "f19cfd3b106f4e43ab8fd5b0d101ebe3",
      "e7f3b2beea9b48e48e5adbe15b547be7",
      "e323878741cc4627a3f84c426ee1a154",
      "0df3b50abae24b898cea5d4dbab2b902",
      "8d1455e0b74a4b9bb80887e8931fd997",
      "c6719cad848c4826b1d4a5568beed56b",
      "86a81cff04d146288b4be82852f4093a",
      "91c1b963c31744ccb6504655ba7110f3",
      "5c758dec94504d2db0f8bc04f7fb0985",
      "9099cf47d17043cfbaf8d124773f9a92",
      "2a3d0e93c5484458915471515827c8d6",
      "aa840ba5473341f1ae290edaec6e9736",
      "d563537fbc7543c5af1b90e86262eed6",
      "d852f06474524c7b946717e96df9a79d",
      "da690bde622f46eb9d79b74b40ed48f7",
      "cfcfe298534c44f690fb026012661afb",
      "25cdacc53f5b4067a4588d71e46bf090",
      "f0fc33f002374cac8fde2557c3426348"
     ]
    },
    "executionInfo": {
     "elapsed": 1488711,
     "status": "ok",
     "timestamp": 1755086846246,
     "user": {
      "displayName": "Pavan Kumar",
      "userId": "06100093160327977602"
     },
     "user_tz": -330
    },
    "id": "RvCqDIpGlUPb",
    "outputId": "79489e36-d8ee-499c-ab5d-b81875a0cee0"
   },
   "outputs": [],
   "source": [
    "# !pip install ticktoken\n",
    "\n",
    "import tiktoken\n",
    "import os\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# we go thorugh all stories and every story is converted into token ids and is stored along with its length\n",
    "def process(example):\n",
    "  import tiktoken\n",
    "  enc = tiktoken.get_encoding(\"gpt2\")\n",
    "  ids = enc.encode_ordinary(example[\"text\"])\n",
    "  out = {'ids': ids, 'len': len(ids)}\n",
    "  return out\n",
    "\n",
    "if not os.path.exists(\"train.bin\"):\n",
    "  tokenized = ds.map(process,\n",
    "                     remove_columns={'text'},\n",
    "                     desc=\"tokenising the splits\",\n",
    "                     num_proc=8)\n",
    "\n",
    "  #concat all ids in each dataset into 1 large file for training\n",
    "  for split,dset in tokenized.items():\n",
    "    arr_len = np.sum(dset['len'], dtype=np.uint64)\n",
    "    filename = f'{split}.bin' # create a bin file for each training and validation\n",
    "    dtype = np.uint16 #since enc.max_token_value == 50265 is < 2**16\n",
    "    arr = np.memmap(filename, dtype=dtype, mode='w+', shape=(arr_len,)) # this is the memory mapped array\n",
    "    total_batches = 1024 #entire dataset is split into 1024 batches\n",
    "\n",
    "    idx = 0\n",
    "    for batch_idx in tqdm(range(total_batches), desc=f'writing {filename}'):\n",
    "      #batch samples for faster write\n",
    "      batch = dset.shard(num_shards=total_batches, index=batch_idx, contiguous=True).with_format('numpy')\n",
    "      arr_batch = np.concatenate(batch['ids']) # for each batch token ids are collected and stored into the array\n",
    "      # write into mmap\n",
    "      arr[idx:idx+len(arr_batch)] = arr_batch\n",
    "      idx += len(arr_batch)\n",
    "    arr.flush() #stores everything on disc finally"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ak-o4pWN0slv"
   },
   "source": [
    "## Step 3: create input output pairs from dataset\n",
    "\n",
    "The loss function used for training the model is finally based on these inuput, output pairs.\n",
    "In regression or classification tasks in ML the datasets have the input and the correct output, but in language modelling we create them from the dataset itself.<br>\n",
    "Traditional language model predict the next token given the exisiting sequence of tokens. (next token prediction)\n",
    "\n",
    "To create these pairs\n",
    "1. fix context size: max length of tokens the model looks at, at a time to predict the next token. Chunk the entire dataset according to the context size. (in our case each chunk conains 4 token ids).\n",
    "2. Batch size: training is done for each of the batches, loss is backpropagated per batch after calculating batch loss.\n",
    "\n",
    "Finally input batches are created like matrices with <br>\n",
    "number of rows = batch size <br>\n",
    "number of cols = context size <br>\n",
    "\n",
    "The output batch is then created by just sliding the input window/token sequence by 1 token.\n",
    "\n",
    "When we look at a training pair we have 4 prediction tasks here and not just 1.\n",
    "\n",
    "Training objective is then just token prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1755086846251,
     "user": {
      "displayName": "Pavan Kumar",
      "userId": "06100093160327977602"
     },
     "user_tz": -330
    },
    "id": "R7KuMIEO1ZaY"
   },
   "outputs": [],
   "source": [
    "def get_batch(split): # this function just returns 1 input matrix and 1 output matrix\n",
    "  if split == 'train':\n",
    "    data = np.memmap('train.bin', dtype=np.uint16, mode='r')\n",
    "  else:\n",
    "    data = np.memmap('validation.bin', dtype=np.uint16, mode='r')\n",
    "  ix = torch.randint(len(data) - block_size, (batch_size,)) #randomly samples 4 (batch size) integers; block_size = context size\n",
    "  x = torch.stack([torch.from_numpy((data[i:i+block_size]).astype(np.int64)) for i in ix]) #stacks together x1, x2, x3 and x4\n",
    "  y = torch.stack([torch.from_numpy((data[i+1:i+1+block_size]).astype(np.int64)) for i in ix]) #stacks together y1, y2, y3 and y4\n",
    "  #for computational efficiency\n",
    "  if device_type == 'cuda':\n",
    "    x, y = x.pin_memory().to(device, non_blocking=True), y.pin_memory().to(device, non_blocking=True)\n",
    "  else:\n",
    "    x, y = x.to(device), y.to(device)\n",
    "  return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UWUJ7_N37uLz"
   },
   "source": [
    "- pin memory locks memory of tensor in RAM allowing for fatser transfer of tensor to GPU.\n",
    "- .to(device) usually blocks the CPU until copying is done. non_blocking = true ensures CPU can continue other work (like next batch preparation)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PFRpnbVvatmJ"
   },
   "source": [
    "## Step 4: SLM model architecture\n",
    "\n",
    "The architecture consists of 3 parts\n",
    "1. Input block\n",
    "2. Processor\n",
    "3. Output block\n",
    "\n",
    "![alt text](images/transformer_block.png)\n",
    "\n",
    "The journey of 1 row from the batch:\n",
    "1. Token embedding: The token ids of context length are converted into higher dimensional vectors. This is required because every word carries some meaning and contributes to the sentences. Words have some semantic notion and hence they are represented in some higher dimensional spaces that capture some info of the meaning and relationships. Similar tokens have vectors closer together than the ones with opposite or unrelated tokens.<br>\n",
    "Done with Token embedding matrix with a fixed embedding dimension for each of the token/embedding vector, in number tokens = number of words in our vocabulary.<br>\n",
    "The matrix is then just used as a lookup table to retrieve the embedding corresponding to the token id.\n",
    "The embeddings are initialised randomly and can be trained through backpropagation like the network weights.\n",
    "\n",
    "2. Layer norm prevents the internal covariate shift and makes training easier.\n",
    "\n",
    "3. Multihead attention: captures how one token is related to the other token. It augments the input embedding vector, so that for every vector we have the information about the neighbouring vectors. This block converts the input embedding vector into a context vector, by which we take into account how the token relates to all the other tokens and then the calculated scores are used to go from input embedding vector to the context vector.<br>\n",
    "This context vector is much richer as it has information of how it relates to tokens surrounding it.<br>\n",
    "When we look at attention scores, causality should exist. Ideally every token should have an attention score with only with that token and the token appearing before it and not after it. Hence we set the attention scores to 0 above the diagonal of the attention scores matrix. This is known as causal attention.\n",
    "Context capturing is offloaded onto the trainable weight matrices q (query), k (key) and v (value) matrices as we could not device any other method to capture context.\n",
    "\n",
    "4. Feed fwd network (MLP): is like an expansion-compression neural network, with input dimension same as output dimension. input and output layer's dim = 768; hidden layer dim = 4*768=3072. Due to expansion + contraction and the additional trainable parameters the model learns to explore a much richer space to learn new things. Without this network the model cant learn about context, patterns underlying the data and cant answer queries well. Addition of this network changes the performance of the model. Activation function used here is Gelu, shown to have good results here.\n",
    "\n",
    "5. output network layer (output head):converts every vector to vocabulary size. Output is the logits tensor.\n",
    "1 batch: 1(batch size)\\*4(number of tokens)\\*50275(vocabsize)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qFjG_LUlmyfb"
   },
   "source": [
    "LayerNorm(x) = x-μ / \\sqrt{σ^2 + ϵ} * weight + bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1755087361708,
     "user": {
      "displayName": "Pavan Kumar",
      "userId": "06100093160327977602"
     },
     "user_tz": -330
    },
    "id": "OPtvIDG1auAG"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "from dataclasses import dataclass\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "from contextlib import nullcontext\n",
    "import os\n",
    "\n",
    "class LayerNorm(nn.Module):\n",
    "  def __init__(self,ndim, bias):\n",
    "    super().__init__()\n",
    "    self.weight = nn.Parameter(torch.ones(ndim))\n",
    "    self.bias = nn.Parameter(torch.zeros(ndim)) if bias else None\n",
    "  def forward(self,x):\n",
    "    return F.layer_norm(x, self.weight.shape, self.weight, self.bias, 1e-5) #normalises across specified last N dimensions, then optionally applies scaling(weight) and shifting (bias) parameters.\n",
    "    # x = input tensor to normalise\n",
    "    # self.weight.shape = dimension to normalise over\n",
    "    # weight and bias are learnable parameters  (γ, β in layer norm equation)\n",
    "    # 1e-5 added to variance to avoid division by zero\n",
    "\n",
    "class CausalSelfAttention(nn.Module): #self attention because we calc attention of the tokens of the sentence with themselves.\n",
    "  def __init__(self,config):\n",
    "    super().__init__()\n",
    "    assert config.n_embd % config.n_head == 0\n",
    "    self.c_attn = nn.Linear(config.n_embd, 3*config.n_embd, bias=config.bias)\n",
    "    self.c_proj = nn.Linear(config.n_embd, config.n_embd, bias = config.bias)\n",
    "    self.attn_dropout = nn.Dropout(config.dropout)\n",
    "    self.resid_dropout = nn.Dropout(config.dropout)\n",
    "    self.n_head = config.n_head\n",
    "    self.n_embd = config.n_embd\n",
    "    self.flash = hasattr(F, 'scaled_dot_product_attention')\n",
    "    if not self.flash:\n",
    "      self.register_buffer(\"bias\",torch.tril(torch.ones(config.block_size, config.block_size))\n",
    "                           .view(1, 1, config.block_size, config.block_size))\n",
    "  def forward(self,x):\n",
    "    B, T, C = x.size()\n",
    "    q, k, v = self.c_attn(x).split(self.n_embd, dim=2)\n",
    "    q = q.view(B, T, self.n_head, C//self.n_head).transpose(1,2) # query matrix\n",
    "    k = k.view(B, T, self.n_head, C//self.n_head).transpose(1,2) # key matrix\n",
    "    v = v.view(B, T, self.n_head, C//self.n_head).transpose(1,2) # value matrix\n",
    "\n",
    "    if self.flash:\n",
    "      y = F.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=self.attn_dropout.p if self.training else 0.0, is_causal=True)\n",
    "    else:\n",
    "      att = (q @ k.transpose(-2,-1)) ** (1.0 / math.sqrt(k.size(-1))) # attention score matrix (q*k^Transpose). divide by sqrt(keys_dimention) to contain the variance of the dot product, to stabilise the training.\n",
    "      att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf')) # implements causal attention. -inf becomes 0 after softmax\n",
    "      att = F.softmax(att, dim=-1) # attention score matrix is converted to an attention weights matrix. Every row sums up to 1.\n",
    "      att = self.attn_dropout(att) #improves generalisation\n",
    "      y = att @ v # attention weights matrix * values = context vector matrix\n",
    "\n",
    "    y = y.transpose(1,2).contiguous().view(B, T, C)\n",
    "    y = self.resid_dropout(self.c_proj(y)) #output projection layer. Final neural n/w added at the output. Preserves the shape of the output. + 1 more layer of dropout\n",
    "    return y\n",
    "\n",
    "class MLP(nn.Module): # expansion and contraction neural n/w. These additional trainable params allow neural net to explore a much bigger space.\n",
    "  def __init__(self, config):\n",
    "    super().__init__()\n",
    "    self.c_fc = nn.Linear(config.n_embd, 4 * config.n_embd, bias=config.bias) # hidden layer has a dimension of 4 * embedding dim\n",
    "    self.gelu = nn.GELU()\n",
    "    self.c_proj = nn.Linear(4 * config.n_embd, config.n_embd, bias=config.bias) # o/p layer again has dim of embedding dim/input-layer\n",
    "    self.dropout = nn.Dropout(config.dropout)\n",
    "\n",
    "  def forward(self,x):\n",
    "    return self.dropout(self.c_proj(self.gelu(self.c_fc(x))))\n",
    "\n",
    "class Block(nn.Module): #implements the transformer block by assembling the layer norm, multihead attention and Feed Fwd nn\n",
    "  def __init__(self, config):\n",
    "    super().__init__()\n",
    "    self.ln1 = LayerNorm(config.n_embd, bias=config.bias)\n",
    "    self.attn = CausalSelfAttention(config)\n",
    "    self.ln2 = LayerNorm(config.n_embd, bias=config.bias)\n",
    "    self.mlp = MLP(config)\n",
    "  def forward(self, x):\n",
    "    x  = x + self.attn(self.ln1(x)) # skip connections added to avoid vanishing gradients by providing the gradient an alternative path to flow. (Like in ResNet)\n",
    "    x = x + self.mlp(self.ln2(x))\n",
    "    return x\n",
    "\n",
    "@dataclass\n",
    "class GPTConfig:\n",
    "  block_size: int\n",
    "  vocab_size: int\n",
    "  n_layer: int\n",
    "  n_head: int\n",
    "  n_embd: int\n",
    "  dropout: float = 0.0\n",
    "  bias: bool = True\n",
    "\n",
    "class GPT(nn.Module):\n",
    "  def __init__(self, config):\n",
    "    super().__init__()\n",
    "    self.config = config\n",
    "    self.transformer = nn.ModuleDict(dict(\n",
    "        wte = nn.Embedding(config.vocab_size, config.n_embd),\n",
    "        wpe = nn.Embedding(config.block_size, config.n_embd),\n",
    "        drop = nn.Dropout(config.dropout),\n",
    "        h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]), # we use multiple transformer blocks, n_layers tell us how many blocks are used\n",
    "        ln_f = LayerNorm(config.n_embd, bias=config.bias)\n",
    "    ))\n",
    "    self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "    self.transformer.wte.weight = self.lm_head.weight #weight tying\n",
    "\n",
    "    self.apply(self._init_weights)\n",
    "    for pn, p in self.named_parameters():\n",
    "      if pn.endswith('c_proj.weight'):\n",
    "        torch.nn.init.normal_(p, mean=0.0, std=0.02/math.sqrt(2 * config.n_layer))\n",
    "\n",
    "  def _init_weights(self, module): # different initialisations done for different sections of the n/w\n",
    "    if isinstance(module, nn.Linear):\n",
    "      torch.nn.init.normal_(module.weight, mean=0.0, std=0.02) # gaussian init\n",
    "      if module.bias is not None:\n",
    "        torch.nn.init.zeros_(module.bias)\n",
    "    elif isinstance(module, nn.Embedding):\n",
    "      torch.nn.init.normal_(module.weight, mean=0.0, std=0.02) # gaussian init\n",
    "\n",
    "  def forward(self, idx, targets=None):\n",
    "    device = idx.device\n",
    "    b, t = idx.size()\n",
    "    assert t <= self.config.block_size, \"context length exceeded\"\n",
    "    pos = torch.arange(0, t, dtype=torch.long, device=device)\n",
    "\n",
    "    tok_emb = self.transformer.wte(idx) #token embedding\n",
    "    pos_emb = self.transformer.wpe(pos) #position embedding\n",
    "    x = self.transformer.drop(tok_emb + pos_emb) # the sum of both embeddings are passed as input to the transformer blocks\n",
    "    for block in self.transformer.h: # multiple transformer blocks are used by the GPT\n",
    "      x = block(x)\n",
    "    x = self.transformer.ln_f(x) # final layer norm here before the output layer\n",
    "\n",
    "    if targets is not None:\n",
    "      logits = self.lm_head(x) # (output head)neural network with number of rows = embedding size & n.o of cols = vocab size.\n",
    "      loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1) # the predicted token is compared with actual target tokens and cross entropy loss is calculated.\n",
    "      return logits, loss\n",
    "    else:\n",
    "      logits = self.lm_head(x)\n",
    "      return logits, None # softmax over logits used for predicting the next output. cross entropy loss used when training.\n",
    "\n",
    "  @torch.no_grad()\n",
    "  def generate(self, idx, max_new_tokens, temperature=1.0, top_k=None):\n",
    "    for _ in range(max_new_tokens):\n",
    "      idx_cond = idx if idx.size(-1) <= self.config.block_size else idx[:,-self.config.block_size:]\n",
    "      logits, _ = self(idx_cond)\n",
    "      logits = logits[:,-1,:] / temperature\n",
    "      if top_k is not None:\n",
    "        v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
    "        logits[logits < v[:, [-1]]] = -float('Inf')\n",
    "      probs = F.softmax(logits, dim=-1)\n",
    "      idx_next = torch.multinomial(probs, num_samples=1)\n",
    "      idx = torch.cat((idx, idx_next), dim=1)\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 840,
     "status": "ok",
     "timestamp": 1755087367214,
     "user": {
      "displayName": "Pavan Kumar",
      "userId": "06100093160327977602"
     },
     "user_tz": -330
    },
    "id": "gH36Adea32sQ"
   },
   "outputs": [],
   "source": [
    "config = GPTConfig(\n",
    "    vocab_size=50257,     # use the tokenizer's vocab size\n",
    "    block_size=128,       # or whatever context size you're training with\n",
    "    n_layer=6,            # number of transformer blocks we have\n",
    "    n_head=6,             # number of attention heads we have in the attention blocks\n",
    "    n_embd=384,           # embedding dimension\n",
    "    dropout=0.1,\n",
    "    bias=True\n",
    ")\n",
    "\n",
    "model = GPT(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v9k-mCjHrtbc"
   },
   "source": [
    "weight initialisation for different layers:\n",
    "![alt text](images/weight_initialisations.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YOYDamlas8R4"
   },
   "source": [
    "## Step 5: Define the loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3DRMTu10Qk5o"
   },
   "source": [
    "From logits of the nn we get the output probabilities of the true target token IDs. We want these probabilities to be as close as possible to 1. The loss function we define and use is also based on these probabilities. We use NLL here with these probabilities. -1/4(log p1 + log p2 + log p3 + log p4...). Since we want to maximise p1, p2, ... we minimise the NLL. For the NLL to be min, the probabilities p1, p2, ... have to be close to 1. This NLL is also called cross entropy loss, through which backpropagation is done for language models. Taking loss of p1*p2*p3*p4 is not ideal as we would want to calculate the derivative and calculating the derivative of products isnt easy, hence we take log and then taking the derivative of the sum of log is easier for back propagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1755087374340,
     "user": {
      "displayName": "Pavan Kumar",
      "userId": "06100093160327977602"
     },
     "user_tz": -330
    },
    "id": "ybl1cpnwtikZ"
   },
   "outputs": [],
   "source": [
    "def estimate_loss(model): #gets batches and calculates the loss for those few batchs\n",
    "  out = {}\n",
    "  model.eval()\n",
    "  with torch.inference_mode():\n",
    "    for split in ['train', 'val']:\n",
    "      losses = torch.zeros(eval_iters)\n",
    "      for k in range(eval_iters):\n",
    "        x, y = get_batch(split)\n",
    "        with ctx:\n",
    "          logits, loss = model(x, y)\n",
    "        losses[k] = loss.item()\n",
    "      out[split] = losses.mean()\n",
    "  model.train()\n",
    "  return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CpGu7lujWAxA"
   },
   "source": [
    "## Step 6: Define the training loop\n",
    "\n",
    "1. Use autocast to make the training faster. Enables automatic mixed precision (AMP), where model uses float16 where it is safe and float32 where needed.\n",
    "![alt text](images/AMP.png)\n",
    "\n",
    "2. Gradient accumulation: When we want to train with a batch size of 1024, but GPU only fits a batch of 32 then we set\n",
    "\n",
    "    gradient_accumulation_steps = 32\n",
    "\n",
    "    run 32 iterations with batch_size = 32\n",
    "\n",
    "    accumulate gradients during loss.backward()\n",
    "    then do optimiser.step() after 32 steps. {PS: 32 * 32 = 1024}\n",
    "\n",
    "    Parameters are updated after 32 steps with the final accumulated gradient, no mean is used here.\n",
    "\n",
    "3. AdamW optimiser used with learning rate with a warmup and then a cosine decay regime.\n",
    "![alt text](images/learning_rate.png)\n",
    "\n",
    "4. Pretraining the SLM<br>\n",
    "    For each of the training iteration we perform the following steps:<br>\n",
    "    choose x & y -> pass x through model to get logits -> compute loss b/w logits and y (Cross Entropy) -> Backpropagate loss -> Accumulate gradients till we reach gradient accumulation steps -> update parameters -> update LR -> evaluate and save best\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 273,
     "status": "ok",
     "timestamp": 1755087379089,
     "user": {
      "displayName": "Pavan Kumar",
      "userId": "06100093160327977602"
     },
     "user_tz": -330
    },
    "id": "yTpHA7dQb3_n",
    "outputId": "81e7c4b6-5a03-4da7-9596-0b83c117edf6"
   },
   "outputs": [],
   "source": [
    "# SLM training configuration\n",
    "\n",
    "import torch\n",
    "from contextlib import nullcontext\n",
    "\n",
    "max_iters = 20000\n",
    "learning_rate = 1e-4\n",
    "warmup_steps = 1000\n",
    "min_lr = 5e-4\n",
    "eval_iters = 500\n",
    "batch_size = 32\n",
    "block_size = 128 #context size\n",
    "\n",
    "gradient_accumulation_steps = 32\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device_type = 'cuda' if 'cuda' in device else 'cpu'#for use in autocast\n",
    "\n",
    "dtype = 'bfloat16' if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else 'float16'\n",
    "ptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[dtype]\n",
    "\n",
    "ctx = nullcontext() if device_type =='cpu' else torch.amp.autocast(device_type=device_type, dtype=ptdtype)\n",
    "\n",
    "torch.set_default_device(device)\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5484,
     "status": "ok",
     "timestamp": 1755087389738,
     "user": {
      "displayName": "Pavan Kumar",
      "userId": "06100093160327977602"
     },
     "user_tz": -330
    },
    "id": "Pxss7JidLgXl",
    "outputId": "3820159f-0f8c-40df-d542-d3ad94ffc3f4"
   },
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import LinearLR, SequentialLR, CosineAnnealingLR\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, betas=(0.9,0.95), weight_decay=0.1, eps=1e-9) #weight decay for regularisation\n",
    "\n",
    "scheduler_warmup = LinearLR(optimizer, total_iters=warmup_steps) #linear warmup\n",
    "scheduler_decay = CosineAnnealingLR(optimizer, T_max = max_iters - warmup_steps, eta_min = min_lr) #cosine decay\n",
    "scheduler = SequentialLR(optimizer, [scheduler_warmup, scheduler_decay], milestones=[warmup_steps]) #combines both\n",
    "\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16')) #for mixed precision\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 307,
     "referenced_widgets": [
      "2b942600d0cb43aba60258eeb733b777",
      "22acd46af28b44358378847dbe4fa871",
      "902f80cc89784ef496da79a1d61eb68b",
      "aa7ea89488664be3a41f0630695bcd14",
      "ed862733b0fa4318b705a0b6e2286773",
      "8fdc6b5338384ae5ac211a2f635b9c4d",
      "be3a06dd88f84c068336695489586d50",
      "eed6d949d8934779a9ced18238ec9642",
      "598932123b7d4dc389bd3f08a5e69a3a",
      "f4eac3f3a8f349998f8e84053c0e90a8",
      "e56fe66d8f38415089f9699c6ac2419f"
     ]
    },
    "executionInfo": {
     "elapsed": 11808,
     "status": "error",
     "timestamp": 1755087406151,
     "user": {
      "displayName": "Pavan Kumar",
      "userId": "06100093160327977602"
     },
     "user_tz": -330
    },
    "id": "hRgZvCLXNVpB",
    "outputId": "5456397c-b99e-4bc4-f4f5-af7fedd473ef"
   },
   "outputs": [],
   "source": [
    "# pretraining the SLM\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "best_model_params_path = \"best_model_params.pt\"\n",
    "train_loss_list, validation_loss_list = [], []\n",
    "\n",
    "# Ensure model is on the correct device\n",
    "model = model.to(device)\n",
    "\n",
    "# In your training loop\n",
    "for epoch in tqdm(range(max_iters)):\n",
    "    if epoch % eval_iters == 0 and epoch != 0:\n",
    "        # Ensure estimate_loss uses the correct device\n",
    "        losses = estimate_loss(model)\n",
    "        print(f\"Epoch {epoch}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "        print(f\"The current learning rate: {optimizer.param_groups[0]['lr']:.5f}\")\n",
    "        train_loss_list += [losses['train']]\n",
    "        validation_loss_list += [losses['val']]\n",
    "\n",
    "        if losses['val'] < best_val_loss:\n",
    "            best_val_loss = losses['val']\n",
    "            torch.save(model.state_dict(), best_model_params_path)\n",
    "\n",
    "    # Ensure X and y are on the correct device\n",
    "    X, y = get_batch(\"train\")\n",
    "    X, y = X.to(device), y.to(device)\n",
    "\n",
    "    with ctx:\n",
    "        logits, loss = model(X, y)\n",
    "        loss = loss / gradient_accumulation_steps\n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "    if ((epoch + 1) % gradient_accumulation_steps == 0) or (epoch + 1 == max_iters):\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.5)\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fsvo0JZPNtNf"
   },
   "source": [
    "Plotting the SLM loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1555570,
     "status": "aborted",
     "timestamp": 1755086846320,
     "user": {
      "displayName": "Pavan Kumar",
      "userId": "06100093160327977602"
     },
     "user_tz": -330
    },
    "id": "XrsI0j7jNyHv"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "train_loss_list_converted = [i.cpu().detach() for i in train_loss_list]\n",
    "validation_loss_list_converted = [i.cpu().detach() for i in validation_loss_list]\n",
    "\n",
    "plt.plot(train_loss_list_converted, 'g', label='train_loss')\n",
    "plt.plot(validation_loss_list_converted, 'r', label='validation_loss')\n",
    "plt.xlabel(\"Steps - Every 100 epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fLvLwAAPeAG0"
   },
   "source": [
    "## Step 7: Inference\n",
    "\n",
    "loop over\n",
    "1. Index of highest value taken\n",
    "2. Token ID extracted\n",
    "3. Decoded back to text\n",
    "4. Produces next token\n",
    "5. Append to previous token\n",
    "6. Pass to model and cont. loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1555571,
     "status": "aborted",
     "timestamp": 1755086846322,
     "user": {
      "displayName": "Pavan Kumar",
      "userId": "06100093160327977602"
     },
     "user_tz": -330
    },
    "id": "NifJxNkwe0_C"
   },
   "outputs": [],
   "source": [
    "#Load the model\n",
    "model = GPT(config)  # re-create the model with same config\n",
    "device =  \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "best_model_params_path = \"best_model_params.pt\"\n",
    "model.load_state_dict(torch.load(best_model_params_path, map_location=torch.device(device))) # load best model states\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1555572,
     "status": "aborted",
     "timestamp": 1755086846323,
     "user": {
      "displayName": "Pavan Kumar",
      "userId": "06100093160327977602"
     },
     "user_tz": -330
    },
    "id": "SzP_B366OBHk"
   },
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "sentence = \"Once upon a time there was a pumpkin.\"\n",
    "enc = tiktoken.get_encoding(\"gpt2\")\n",
    "context = (torch.tensor(enc.encode_ordinary(sentence)).unsqueeze(dim = 0))\n",
    "y = model.generate(context,200)\n",
    "print(enc.decode(y.squeeze().tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1555573,
     "status": "aborted",
     "timestamp": 1755086846324,
     "user": {
      "displayName": "Pavan Kumar",
      "userId": "06100093160327977602"
     },
     "user_tz": -330
    },
    "id": "9owLo8YoOLAc"
   },
   "outputs": [],
   "source": [
    "# from google.colab import runtime\n",
    "# runtime.unassign()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyMRMabirZXdcq46stelIUHY",
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "SLM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
